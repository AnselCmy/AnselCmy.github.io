{"meta":{"title":"Ansel's Site","subtitle":null,"description":null,"author":"Ansel Chen","url":"http://yoursite.com"},"pages":[{"title":"","date":"2017-10-13T01:42:30.000Z","updated":"2017-10-13T01:42:30.000Z","comments":true,"path":"NN_by_Hand.html","permalink":"http://yoursite.com/NN_by_Hand.html","excerpt":"","text":"手工实现简单神经网络我们在这里对于整个手工实现神经网络的逻辑是，先初始化整个类12345678910111213141516171819202122接下来我们对这个需要手工实现的神经网络类。### 所需要```__init__```的变量因为我们在进行正向传播和反向传播的过程中，需要保存下过程中所计算的值，在实作的过程中，我们通过```list```的结构进行保存，这样的好处在于我们可以直接通过index的方式找到相应index的层的值，例如，```W[1]```可以表示神经网络第一层的```W```参数，我们通过以下的代码进行初始化 ```pythonclass nn_model: def __init__(self, layers_dims): self.layers_dims = layers_dims self.L = len(self.layers_dims)-1 # sub 1 for input layer # variable of Z and A, we don&apos;t use the index 0 self.A = [None for i in range(self.L+1)] self.Z = [None for i in range(self.L+1)] self.dA = [None for i in range(self.L+1)] self.dZ = [None for i in range(self.L+1)] # parameter of W and b, we don&apos;t use the index 0 self.W = [None for i in range(self.L+1)] self.b = [None for i in range(self.L+1)] self.dW = [None for i in range(self.L+1)] self.db = [None for i in range(self.L+1)] 我们把神经网络放在一个5, 2]```就表示一个三层的神经网络，从第一层到第三层每一层的神经元的个数是3，5，2。123456789101112131415161718192021222324252627我们对构造函数的每一个值进行解释 | 变量 | 定义 || ------| ------ | |```self.L```|神经网络的层数，是去掉输入层的层数||```self.A```|每一层的a值的列表，也就是经过激活函数的值，但是第0层是输入值，也就是X||```self.Z```|每一层的z值的列表，也就是经过线性函数的值，第0层没有，所以我们空着||```self.dA```|每一层的a值的对于损失函数的导数的列表，也就是\\\\(\\frac&#123;dJ&#125;&#123;da&#125;\\\\)的列表||```self.dZ```|每一层的z值的对于损失函数的导数的列表，也就是\\\\(\\frac&#123;dJ&#125;&#123;dz&#125;\\\\)的列表||```self.W```|每一层的w值矩阵的列表，也就是对进行从a到z的线性变换函数的系数||```self.b```|每一层的b值向量的列表，也就是对进行从a到z的线性变换函数的偏置||```self.dW```|每一层的w值的对于损失函数的导数的列表，也就是\\\\(\\frac&#123;dJ&#125;&#123;dW&#125;\\\\)的列表||```self.db```|每一层的b值的对于损失函数的导数的列表，也就是\\\\(\\frac&#123;dJ&#125;&#123;db&#125;\\\\)的列表|可以看出我们在初始化例如```W```这一类参数的列表的时候，并没有因为在第0层没有这样的参数，而少初始化一层，我们恰恰初始化了第0层的参数为```None```，但是我们并没有用，因为我们想在后面使用```W[1]```的时候就表示第一层的W值，而不是要用```W[0]```很蹩脚的表示第一层。### 初始化参数```pythondef initial_params(self): for i in range(1, self.L+1): # iter from 1 ~ L layer self.W[i] = np.random.randn(self.layers_dims[i], self.layers_dims[i-1])*0.01 self.b[i] = np.zeros((self.layers_dims[i], 1)) # take a assert for the certain dimension assert(self.W[i].shape == (self.layers_dims[i], self.layers_dims[i-1])) assert(self.b[i].shape == (self.layers_dims[i], 1)) 将参数初始化的时候，我们需要注意只需要初始化第1到L层的123```参数只是简单的用随机数初始化，并且乘0.01为了防止刚开始初始化的```W```参数太大而导致梯度下降的时候效果不明显，因为我们之后输出层要使用到```sigmoid```作为激活函数，因为这里只是简单的实现，我们更多的是理解反向传播，梯度下降，并没有在意太多的优化算法。### 赋值训练数据```X, Y 123456789def set_data(self, X, Y): self.X = X self.Y = Y self.A[0] = X self.m = X.shape[1] # X.shape[0] is the n_x assert(X.shape[0] == self.W[1].shape[1]) # Y.shape[0] is the n_y assert(Y.shape[0] == self.W[-1].shape[0]) 我们赋值了self.Y```之后，将X放到了```self.A[0]```，也就是输入层。然后获取了训练数据的总数也就是```self.m```。12345678910111213141516171819202122232425262728之后的两个```assert```是为了保证我们输入的```X, Y```值和我们之前设置的```self.layers_dims```相匹配。### 正向传播```pythondef linear_activation_forward(self, A_prev, W, b, activation): &quot;&quot;&quot; Arguments: A_prev: the A(activated) value of previous layer W: the parameter &apos;W&apos; of current layer b: the parameter &apos;b&apos; of current layer activation: the activation function used in neuron Returns: A = the activated value computed by A_prev and the parameters Z = the value after linear function &quot;&quot;&quot; Z = np.dot(W, A_prev) + b # the linear part if(activation == &apos;sigmoid&apos;): A = self.sigmoid(Z) elif(activation == &apos;relu&apos;): A = self.relu(Z) assert(Z.shape == (W.shape[0], A.shape[1])) # (n_h, m) assert(A.shape == Z.shape) return Z, A 我们现在介绍的函数是在某一层进行正向传播所需要的动作，包括线性函数和激活函数，我们在这个函数外面再打包一层，就是所有层的正向传播。 12345678910111213def forward(self): # iter from 1 ~ L-1 layer for i in range(1, self.L): A_prev = self.A[i-1] W = self.W[i] b = self.b[i] self.Z[i], self.A[i] = self.linear_activation_forward(A_prev, W, b, 'relu') # the last L layer, sigmoid for binary classifier A_prev = self.A[self.L-1] W = self.W[self.L] b = self.b[self.L] self.Z[self.L], self.A[self.L] = self.linear_activation_forward(A_prev, W, b, 'sigmoid') 计算cost我们使用交叉熵损失函数计算cost$$cost = \\frac{1}{m}\\sum_{m}{ylog(\\hat{y})} + (1-y)log(1-\\hat{y}) \\tag{1}$$ 123456def compute_cost(self): # beacause here is a binary classifier so the output n_y == 1 cost = (-1./ self.m)*np.sum(self.Y*np.log(self.A[-1]) + (1-self.Y)*np.log(1-self.A[-1]), axis=1) self.cost = np.squeeze(cost) self.J.append(self.cost) assert(self.cost.shape == ()) 反向传播1234567def linear_activation_backward(self, dZ_next, Z, W_next, activation): dA = np.dot(W_next.T, dZ_next) if activation == 'sigmoid': dZ = dA*self.sigmoid_prime(Z) elif activation == 'relu': dZ = dA*self.relu_prime(Z) return dA, dZ 这里的函数是介绍了我们知道dA[i]```的过程，我们再进行一层打包，就可以将函数变成如下的```self.backward()```对每一层进行反向传播并计算变量对损失函数的导数。 123456789101112131415161718192021222324相关反向传播的公式如下： 对于最后一层，因为我们使用的是```sigmoid```最为最后一层的激活函数$$\\frac&#123;dJ&#125;&#123;dW^&#123;[l]&#125;&#125; = \\frac&#123;1&#125;&#123;m&#125;A^&#123;[l-1]&#125;\\frac&#123;dJ&#125;&#123;dZ^&#123;[l]&#125;&#125; \\tag&#123;2&#125;$$$$\\frac&#123;dJ&#125;&#123;db^&#123;[l]&#125;&#125; = \\frac&#123;1&#125;&#123;m&#125;\\frac&#123;dJ&#125;&#123;dZ^&#123;[l]&#125;&#125; \\tag&#123;19&#125;$$```python def backward(self): # the last dZ is often computed by hand self.dZ[-1] = self.A[-1] - self.Y self.dW[-1] = (1./self.m) * np.dot(self.dZ[-1], self.A[-2].T) self.db[-1] = (1./self.m) * np.sum(self.dZ[-1], axis=1, keepdims=True) # iter from L-1 to 1 layer for i in range(self.L-1, 0, -1): dZ_next = self.dZ[i+1] Z = self.Z[i] W_next = self.W[i+1] self.dA[i], self.dZ[i] = self.linear_activation_backward(dZ_next, Z, W_next, &apos;relu&apos;) # dw, db self.dW[i] = (1./self.m) * np.dot(self.dZ[i], self.A[i-1].T) self.db[i] = (1./self.m) * np.sum(self.dZ[i], axis=1, keepdims=True) assert(self.dW[i].shape == self.W[i].shape) assert(self.db[i].shape == self.b[i].shape)"},{"title":"","date":"2017-11-21T16:30:17.000Z","updated":"2017-11-21T16:30:17.000Z","comments":true,"path":"mac下安装openCV3(python3.6).html","permalink":"http://yoursite.com/mac下安装openCV3(python3.6).html","excerpt":"","text":"先使用brew下载opencv3 1$ brew install opencv3 --with-python3 将opencv3的site-package链接到相应的python的site-package 1echo /usr/local/opt/opencv3/lib/python3.6/site-packages &gt;&gt; /usr/local/lib/python3.6/site-packages/opencv3.pth"}],"posts":[{"title":"IntelliJ IDEA配置SpringMVC框架","slug":"IntelliJ_IDEA_SpringMVC","date":"2017-12-24T16:00:00.000Z","updated":"2017-12-25T10:11:29.000Z","comments":true,"path":"2017/12/25/IntelliJ_IDEA_SpringMVC/","link":"","permalink":"http://yoursite.com/2017/12/25/IntelliJ_IDEA_SpringMVC/","excerpt":"","text":"新建项目 首先新建一个project，需要选择SpringMVC，这样idea可以直接导入一些需要的库 配置文件 新建完成之后项目结构如下 修改web.xml 12345678910111213141516171819202122&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\" version=\"3.1\"&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 修改dispatcher-servle.xml 123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\"&gt; &lt;context:component-scan base-package=\"com.test\"/&gt; &lt;mvc:default-servlet-handler/&gt; &lt;mvc:annotation-driven/&gt; &lt;bean id=\"jspViewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"viewClass\" value=\"org.springframework.web.servlet.view.JstlView\"/&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/jsp/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 其中 1&lt;context:component-scan base-package=\"com.test\"/&gt; 是指定一下SpringMVC需要扫描的package，也就是我们等下需要新建的放置controllers的包。 12345&lt;bean id=\"jspViewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"viewClass\" value=\"org.springframework.web.servlet.view.JstlView\"/&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/views/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt;&lt;/bean&gt; 是用来支持jsp的解析，也就是说我们在controllers里面如果用1return \"hello\"; 进行返回，那么SpringMVC就回去/WEB-INF/views/目录下去寻找hello.jsp的文件并且进行渲染 添加Controllers 先在/src下面新建一个com.test.controllers的package，然后新建一个HomeController的java类，结果如下 然后我们在HomeController.java里面添加代码 1234567891011121314151617package com.test.controllers;import org.springframework.stereotype.Controller;import org.springframework.ui.ModelMap;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;@Controller@RequestMapping(value = \"/\", method = RequestMethod.GET)public class HomeController &#123; @RequestMapping(value = \"/home\", method = RequestMethod.GET) public String printHello(ModelMap model) &#123; model.addAttribute(\"param\", \"The is the home page\"); return \"home\"; &#125;&#125; 这段代码是一段基本的处理路由的代码，当我们输入url为http://localhost:8080/home的时候controller就回去渲染响应的home.jsp，并且做了一个页面传参，也就是把命名为param的字符串”The is the home page”传给了页面。 添加Views 在/WEB-INF下添加文件夹views，并且新建.jsp文件home.jsp，结果如下 然后我们在home.jsp添加代码 123456789&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;$&#123;param&#125;&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 注意到其中的${param}就是我们在jsp文件里面获取的之前controller传过来的参数。 添加Tomcat配置 在IDEA里面选择Run-&gt;Edit Configurations…，然后点击左上角的+-&gt;Tomcat Server-&gt;Local，如下 然后点击Application Server的Configure…，在里面选择Tomcat Home，这里需要指定你所需要的Tomcat的所在路径，然后点击OK返回 再选择Deploment，点击+-&gt;Artifact，如下 最后点击OK完成配置 添加jstl 使用Tomcat容器开发，需要添加一些包，在这里下载jakarta-taglibs-standard-1.1.2.zip，解压后将standard.jar和jstl.jar放到lib里面 运行 在运行之前把\\lib复制一份放到\\web\\WEB-INF\\下面 在IDEA里面选择Run-&gt;Run…，选择刚刚配置好的那个Server运行 然后在浏览器中打开http://localhost:8080/home，结果如下","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://yoursite.com/tags/SpringMVC/"},{"name":"IntelliJ IDEA","slug":"IntelliJ-IDEA","permalink":"http://yoursite.com/tags/IntelliJ-IDEA/"}]},{"title":"C++手工实现boxFilter","slug":"boxFilter","date":"2017-12-12T16:00:00.000Z","updated":"2017-12-13T07:16:08.000Z","comments":true,"path":"2017/12/13/boxFilter/","link":"","permalink":"http://yoursite.com/2017/12/13/boxFilter/","excerpt":"","text":"在图像处理中，我们常常用图像滤波的方法来消除图像的噪声，或者提取图像的一些特征。在图像滤波中，方框滤波（boxFilter）是一种常见的线性滤波器。 方框滤波所用到的kernel如下， 可以看出这种滤波的方法非常简单，就是让kernel在原图像中移动然后计算出相应的值放到目标图像中。 这里提供两种手动实现方框滤波的方法。 1.四层for循环实现直观的可以感受到，我们可以用多次的for循环就实现方框滤波，对于原图像的长和宽两层，对于kernel的长和宽有两层。 下面是相关的代码和注释，注意这里虽然说是手动实现，但是相关的基本方法例如读取图片，获得图片像素点等基本的功能还是借用的openCV的函数。 123456789101112131415161718192021222324252627282930313233343536373839404142/* * \\author Ansel Chen * \\e-mail anselcmy@foxmail.com */void boxFilterByHand (InputArray _src, OutputArray _dst, Size ksize)&#123; // 作为boxFilter的kernel，边长必须是奇数并且大于1 assert(ksize.height % 2 == 1 &amp;&amp; ksize.height &gt; 1); assert(ksize.height == ksize.width); // 获得_src的mat Mat src = _src.getMat(); Size srcSize = src.size(); // 对src的图形做拓展padding Mat src_pad; int padding = (ksize.width - 1) / 2; copyMakeBorder(src, src_pad, padding, padding, padding, padding, BORDER_REPLICATE, 0); // 初始化dst，作为结果的mat _dst.create(srcSize, src.type()); Mat dst = _dst.getMat(); // 开始用四层for循环进行卷积的操作 // 首先用两层for循环遍历原图中的每一个点 for (int h = padding; h &lt; srcSize.height+padding; ++h) &#123; for (int w = padding; w &lt; srcSize.width+padding; ++w) &#123; // 考虑到数值的范围，这里需要用int int tempVal = 0; // 再用两层for循环遍历一次kernel的大小，对所选到的点和这点周围的点求和 for (int kh = -padding; kh &lt;= padding; ++kh) &#123; for (int kw = -padding; kw &lt;= padding; ++kw) &#123; tempVal += src_pad.at&lt;uchar&gt;(h+kh, w+kw); &#125; &#125; // 最后求平均，也就是对求和后的值tempVal除以kernel的大小 dst.at&lt;uchar&gt;(h-padding, w-padding) = uchar(tempVal/(ksize.width*ksize.height)); &#125; &#125;&#125; 有以下几点需要注意： kernel也就是窗口是在原图像是从左到右从上到下移动的； 为了保持原图像和目标图像的大小一致，我们这里需要做一个padding，分别在上下左右需要padding出(ksize.width-1)/2的大小，也就是kernel宽度减1再除以2的大小； 我们这里只考虑了简单的灰度图像也就是八位的单通道图像，在openCV里面使用数据类型uchar表示。 2.将行和列分开处理的实现在上面的这一种处理，直觉上已经可以看出四层for循环的效率非常低，仔细观察可以发现，里面有很多重复计算的地方，例如对于某一列来说，进行kernel在原图像某一个位置时和kernel右移时的两次计算时，可以直接先加上新加入的像素点，减去移出窗口的像素点，这样可以避免重复的计算。 我们假设原图像第一行的像素点是$s_1, s_2, s_3 \\cdots \\cdots$，目标图像的第一行像素点是$d_1, d_2, d_3 \\cdots \\cdots$，并且kernel的宽度是3，那么我们有如下的计算方法，$$d_1 = \\frac{s_1 + s_2 + s_3}{3}$$$$d_2 = \\frac{d_1 + s_3 - s_1}{3}$$$$d_3 = \\frac{d_2 + s_4 - s_2}{3}$$$$\\cdots \\cdots$$$$d_n = \\frac{d_{n-1} + s_{n+1} - s_{n-1}}{3}$$只需要对于第一个点用for循环计算出kernel宽度的累加值，之后只需要加上新加入的值，减去移除的值。 具体的C++代码实现如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/* * \\author Ansel Chen * \\e-mail anselcmy@foxmail.com */void boxFilterByHandFast (InputArray _src, OutputArray _dst, Size ksize)&#123; assert(ksize.height % 2 == 1 &amp;&amp; ksize.height &gt; 1); assert(ksize.height == ksize.width); // 获得_src的mat Mat src = _src.getMat(); Size srcSize = src.size(); // 对src的图形做拓展padding Mat src_pad; int padding = (ksize.width - 1) / 2; copyMakeBorder(src, src_pad, padding, padding, padding, padding, BORDER_REPLICATE, 0); // 初始化dst，作为结果的mat _dst.create(srcSize, src.type()); Mat dst = _dst.getMat(); // temp1是将原始图像对每一行进行卷积的操作之后的结果 Mat temp1(src_pad.size().height, dst.size().width, CV_32SC1); // temp2是temp1的转置 Mat temp2(dst.size().width, src_pad.size().height, CV_32SC1); // temp3是将temp2对每一行进行卷积操作之后的结果 Mat temp3(dst.size().width, dst.size().height, CV_32SC1); // 对原图的每一行进行卷积的操作，并且在窗口的移动过程，每次并不用全部重新计算 // 只需要减去窗口现在最左边的像素值，加上最右边的下一个像素值，这样减少了计算量 for(int h = 0; h &lt; temp1.size().height; h++) &#123; int temp_value = 0; // row for (int k = 0; k &lt; ksize.width; k++) &#123; temp_value += (int)src_pad.at&lt;uchar&gt;(h, k); &#125; temp1.at&lt;int&gt;(h, 0) = temp_value; for (int w = 1; w &lt; temp1.size().width; w++) &#123; temp_value += (int)(src_pad.at&lt;uchar&gt;(h, w + ksize.width - 1) - src_pad.at&lt;uchar&gt;(h, w - 1)); temp1.at&lt;int&gt;(h, w) = temp_value; &#125; &#125; // 对每一列进行卷积操作 // 这里我们将之前对行操作的结果进行转置再次重复该操作，就相当于对列操作 transpose(temp1, temp2); for (int h = 0; h &lt; temp3.size().height; h++) &#123; int temp_value = 0; // row for (int k = 0; k &lt; ksize.width; k++) &#123; temp_value += (int)temp2.at&lt;int&gt;(h, k); &#125; temp3.at&lt;int&gt;(h, 0) = temp_value; for (int w = 1; w &lt; temp3.size().width; w++) &#123; temp_value += (int)(temp2.at&lt;int&gt;(h, w + ksize.height - 1) - temp2.at&lt;int&gt;(h, w - 1)); temp3.at&lt;int&gt;(h, w) = temp_value; &#125; &#125; // 最后需要将卷积后得到的值除以kernel的大小 for (int h = 0; h &lt; temp3.size().height; ++h) &#123; for (int w = 0; w &lt; temp3.size().width; ++w) &#123; dst.at&lt;uchar&gt;(w, h) = uchar(temp3.at&lt;int&gt;(h, w) / (ksize.width * ksize.height)); &#125; &#125;&#125; 在上面的实现里面需要注意以下一些问题： 由于我们考虑的是uchar的灰度图像，所以我们使用的中间图像需要使用像素点为int大小的图像防止溢出，在openCV里，相关的参数应该使用CV_32SC1； 这里我们在对每一行进行完相关的操作之后，在对列进行操作的时候先将中间结果进行转置，这样的话相当于再次对行进行了操作，这样做的好处不仅在于简化了代码，而且考虑到了因为CPU架构的一些原因，处理列时会造成cache miss的增加，所以处理行更快。 3.结果对比对于下图，进行了三种boxFilter得到结果， 三种boxFilter的时间对比如下， 123the running time of boxFilterByHand(): 1.079907the running time of boxFilterByHandFast(): 0.024237the running time of cv::boxFilter(): 0.001217","categories":[],"tags":[{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://yoursite.com/tags/Computer-Vision/"}]},{"title":"简单神经网络的数学推导","slug":"NN","date":"2017-10-07T16:00:00.000Z","updated":"2017-12-12T16:23:29.000Z","comments":true,"path":"2017/10/08/NN/","link":"","permalink":"http://yoursite.com/2017/10/08/NN/","excerpt":"","text":"在接触了神经网络的数学方面的推导之后，刚开始感觉非常繁杂，又因为看了很多不同的版本推导，非常混乱，没有把握住核心知识，一直没有机会把这方面的推导公式做一个记录。刚好最近听了Andrew Ng在coursera上的Deep Learning的课程，课程里有非常详细的神经网络反向传播的推导，我就借此机会做一个梳理。 神经网络概述基本的神经网络如下图 图1 我们把 [\\(x_1\\), \\(x_1\\), \\(x_3\\)]称之为神经网络的输入层(input layer)，中间的层称之为隐藏层(hidden layer), 最后一层神经元称之为输出层(output layer)。这里的\\(\\hat{y}\\)表示的是通过神经网络预测的输出值。 输入\\(X\\)经过一些线性变化(linear)和激活函数(activation)的计算最后到达输出层，这一步是前向传播，计算出损失函数之后，反向传播并且对参数从后向前作出相应的变化，再次前向传播，多次迭代。 前向传播(Forwarding)在神经网络里面的前向传播指的是通过\\(W, b\\)等参数对\\(X\\)的线性变化以及激活函数的非线性变化之后，得到输出\\(Y\\)的过程。下面我们详述前向传播的过程。 在描述前向传播之前我们先定义本文的符号表 符号 定义 \\(L\\) 神经元层数 \\(n^{[i]}\\) 第\\(i\\)层的神经元的数量，\\(n^{[0]}\\)为输入的\\(x\\)的数量 \\(a^{[i]}\\) 经过第\\(i\\)层的激活函数后的值，是列向量，\\(a^{[0]}\\)为输入 \\(z^{[i]}\\) 经过第\\(i\\)层的线性变化之后的值，是列向量 \\(w^{[i]}\\) 第\\(i\\)层的w参数，是矩阵 \\(b^{[i]}\\) 第\\(i\\)层的b参数，是列向量 \\(g^{[i]}\\) 第\\(i\\)层的激活函数 \\(\\hat{y}\\) 通过前向传播得到的输出值 \\(J\\) 损失函数 现在我们假设我们的神经网络就是上面的图1所示的神经网络，具有一个隐藏层的双层的神经网络。 首先我们的输入是一个维度是[3, 1]的列向量(column vector)——\\(a^{[0]}\\)，从输入层到第一层的神经网络我们首先会经过一个线性变化为：$$z^{[1]} = w^{[1]}a^{[0]} + b^{[1]} \\tag{1}$$ 在图1的每一个箭头可以被理解成是一次线性变换，比如从\\(x^{[1]}\\)到第一层的第一个神经元，我们就需要进行一次线性变化，所有很容易的知道我们的\\(w^{[1]}\\)应该是一个矩阵，并且维度是\\([n^{[2]}, n^{[1]}]\\)，在这里也就是[4，3]。 我们用向量的方式表达一下(1)式：$$\\left[\\begin{matrix} z^{[1]}_{1,1} \\\\ z^{[1]}_{2,1} \\\\ z^{[1]}_{3,1} \\\\ z^{[1]}_{4,1}\\end{matrix}\\right] =\\left[\\begin{matrix} w^{[1]}_{1,1} &amp; w^{[1]}_{1,2} &amp; w^{[1]}_{1,3} \\\\ w^{[1]}_{2,1} &amp; w^{[1]}_{2,2} &amp; w^{[1]}_{2,3} \\\\ w^{[1]}_{3,1} &amp; w^{[1]}_{3,2} &amp; w^{[1]}_{3,3} \\\\ w^{[1]}_{4,1} &amp; w^{[1]}_{4,2} &amp; w^{[1]}_{4,3}\\end{matrix}\\right]\\left[\\begin{matrix} a^{[0]}_{1,1} \\\\ a^{[0]}_{2,1} \\\\ a^{[0]}_{3,1}\\end{matrix}\\right] +\\left[\\begin{matrix} b^{[1]}_{1,1} \\\\ b^{[1]}_{2,1} \\\\ b^{[1]}_{3,1} \\\\ b^{[1]}_{4,1}\\end{matrix}\\right]\\tag{2}$$ 对于我们这个具体的\\(w^{[1]}\\)做一下分析，比如\\(w^{[1]}_{1,1}\\)就是\\(a^{[0]}_{1,1}\\)到隐藏层的第一个神经元的\\(w\\)参数，\\(w^{[1]}_{1,2}\\)就是\\(a^{[0]}_{2,1}\\)到隐藏层的第一个神经元的\\(w\\)参数，前一层的每一个神经元都会对后一层的每一个神经元产生影响。 经过线性变化后达到神经元后，就会通过激活函数，有\\(z^{[1]}\\)得到\\(a^{[1]}\\)：$$a^{[1]} = g^{[1]}(z^{[1]}) \\tag{3}$$ 这里就完成了从输入层到隐藏层的前向传播，从隐藏层到输出层的前向传播类似如下：$$z^{[2]} = w^{[2]}a^{[1]} + b^{[2]} \\tag{4}$$$$a^{[2]} = g^{[2]}(z^{[2]}) \\tag{5}$$ 到这里我们就完成了所有的前向传播并且得到了一个输出值\\(a^{[2]}\\)，我们下面来详述反向传播。 反向传播(Back Propagation)反向传播是神经网络的基础的重点部分，因为神经网络的优化就是随着反向传播的权值更新而进行的，也就是说，反向传播的作用就是对权值进行更新从而使其可以让输出得到我们想要的效果。 我们之前进行前向传播并且得到了输出值\\(\\hat{y}\\)，根据原本的训练数据的\\(y\\)，我们可以计算出损失函数(loss function)，这里我们使用的交叉熵损失函数(cross entropy)：$$J=L(\\hat{y}, y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y})) \\tag{6}$$预测值和期望值越接近的时候，损失函数接近于0。根据损失函数这一个特性，我们可以知道我们更新权值\\(w\\)和\\(b\\)的目的就是使得\\(L(\\hat{y}, y)\\)减小。通过微积分里面偏导的定义，我们可以知道，对于权值，我们需要做的是让它向使得\\(J\\)变小的方向变化，这里我们引入一个学习率(learning rate)——\\(lr\\)在权值更新的时候，我们这样做：$$w -= lr\\frac{dJ}{dw} \\tag{7}$$$$b -= lr\\frac{dJ}{db} \\tag{8}$$ 我们从后往前来分析反向传播，以下会用到链式法则：$$\\frac{dJ}{da^{[2]}} = -\\frac{d}{da^{[2]}}(ylog\\widehat{y} + (1-y)log(1-\\widehat{y})) \\tag{9}$$$$\\frac{dJ}{dz^{[2]}} = \\frac{dJ}{da^{[2]}}\\frac{da^{[2]}}{dz^{[2]}} = \\frac{dJ}{da^{[2]}}{g^{[2]}}^{\\prime}(z^{[2]}) \\tag{10}$$得到了\\(\\frac{dJ}{da^{[2]}}\\)之后，我们再根据(4)式得到：$$\\frac{dJ}{dw^{[2]}} = a^{[1]}\\frac{dJ}{dz^{[2]}} \\tag{11}$$$$\\frac{dJ}{db^{[2]}} = \\frac{dJ}{dz^{[2]}} \\tag{12}$$我们同时根据以上的计算，计算出前一层所需要的\\(\\frac{dJ}{da^{[1]}}\\)：$$\\frac{dJ}{da^{[1]}} = {w^{[2]}}^{T}\\frac{dJ}{dz^{[2]}} \\tag{13}$$ 得到了\\(\\frac{dJ}{dw^{[2]}}\\)和\\(\\frac{dJ}{dw^{[2]}}\\)之后，就可以对\\(w^{[2]}\\)和\\(b^{[2]}\\)进行更新。 之后也是重复以上的步骤进行反向传播，对每一层的参数进行更新即可。 前向和反向传播在某一层的分析假设我们现在到达了神经网络的第\\(l\\)层，我们根据这层来进行概括性的总结。 对于前向传播，我们肯定知道前一层，也就是\\(l-1\\)层的\\(a\\)值，也就是说\\(a^{[l-1]}\\)已经知，所以我们会有以下的计算：$$z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]} \\tag{14}$$$$a^{[l]} =g^{[l]}(z^{[l]}) \\tag{15}$$这时候就通过\\(a^{[l-1]}\\)计算出了\\(a^{[l]}\\)。 对于反向传播，不同于前向传播以\\(a\\)为线索，我们一般习惯以\\(\\frac{dJ}{dz}\\)为线索来计算，通过最后的损失函数我们可以计算出最后一层的：$$\\frac{dJ}{da^{[L]}} \\tag{16}$$并且通过(15)式我们可以得到：$$\\frac{dJ}{dz^{[L]}} = \\frac{dJ}{da^{[L]}}\\frac{da^{[L]}}{dz^{[L]}} = \\frac{dJ}{da^{[L]}}{g^{[L]}}^{\\prime}(z^{[L]}) \\tag{17}$$ 所以对于反向传播来说，这一层的\\(\\frac{dJ}{dz^{[l]}}\\)我们是已知的，所以我们在第\\(l\\)层的反向传播，需要做以下的计算：$$\\frac{dJ}{dw^{[l]}} = a^{[l-1]}\\frac{dJ}{dz^{[l]}} \\tag{18}$$$$\\frac{dJ}{db^{[l]}} = \\frac{dJ}{dz^{[l]}} \\tag{19}$$$$\\frac{dJ}{da^{[l-1]}} = {w^{[l]}}^{T}\\frac{dJ}{dz^{[l]}} \\tag{20}$$$$\\frac{dJ}{dz^{[l-1]}} = \\frac{dJ}{da^{[l-1]}}\\frac{da^{[l-1]}}{dz^{[l-1]}} = \\frac{dJ}{da^{[l-1]}}{g^{[l-1]}}^{\\prime}(z^{[l-1]}) \\tag{21}$$这时候就由\\(z^{[l]}\\)计算出了\\(z^{[l-1]}\\)。","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Neural Networks","slug":"Neural-Networks","permalink":"http://yoursite.com/tags/Neural-Networks/"}]}]}